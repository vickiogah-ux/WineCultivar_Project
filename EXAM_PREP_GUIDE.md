# ğŸ“ EXAM PREPARATION GUIDE

## Wine Cultivar Prediction Project - Study Material

This guide helps you understand the code and concepts for your exam preparation.

---

## ğŸ“š WHAT YOU'LL LEARN

By studying this project, you'll understand:
1. How to build an end-to-end ML pipeline
2. Data preprocessing and feature scaling
3. Random Forest algorithm and hyperparameters
4. Model evaluation metrics for multiclass classification
5. Model persistence (saving/loading)
6. Web deployment of ML models
7. Building interactive user interfaces with Streamlit

---

## ğŸ” DETAILED CONCEPT BREAKDOWN

### 1. DATA PREPROCESSING

#### What is Data Preprocessing?
Cleaning and preparing raw data for machine learning models.

#### In Our Project:
```python
# Load dataset
wine_data = load_wine()
df = pd.DataFrame(wine_data.data, columns=wine_data.feature_names)

# Check for missing values
df.isnull().sum()

# Select 6 best features out of 11
selected_features = ['alcohol', 'total phenols', 'flavanoids', 
                     'color intensity', 'hue', 'proline']

# Separate features (X) and target (y)
X = df[selected_features]
y = df['cultivar']
```

#### Why It Matters:
- Raw data is often incomplete/noisy
- Feature selection reduces dimensionality
- Better features = better model performance

---

### 2. FEATURE SCALING (StandardScaler)

#### What is Feature Scaling?
Converting features to a standard range (usually mean=0, std=1).

#### The Math:
```
Scaled Value = (Original Value - Mean) / Standard Deviation
```

#### In Our Project:
```python
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
```

#### Why It's Important:
- Alcohol ranges 10-15
- Proline ranges 300-1700
- **Without scaling**: Proline dominates because it's larger
- **With scaling**: All features equally important

#### Feature Statistics Before/After:
| Feature | Mean (Before) | Std (Before) | Mean (After) | Std (After) |
|---------|---------------|--------------|--------------|-------------|
| alcohol | 13.00 | 0.81 | 0.00 | 1.00 |
| proline | 746.89 | 314.91 | 0.00 | 1.00 |

---

### 3. TRAIN-TEST SPLIT

#### What is Train-Test Split?
Dividing data into training (teach model) and testing (evaluate model) sets.

#### In Our Project:
```python
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y,
    test_size=0.2,           # 20% for testing
    random_state=42,         # Reproducibility
    stratify=y              # Balanced split
)
```

#### Why Stratified Split?
Our dataset has 3 classes with different frequencies:
```
Class Distribution:
  Class 1: 59 samples (33%)
  Class 2: 71 samples (40%)
  Class 3: 48 samples (27%)

Without Stratification:
  Test set might get mostly Class 2
  Model seems good but actually biased

With Stratification:
  Test set has 33%, 40%, 27% too
  Fair evaluation across all classes
```

#### Result:
```
Training: 142 samples
Testing:  36 samples
```

---

### 4. RANDOM FOREST CLASSIFIER

#### What is a Decision Tree?
A tree structure that makes decisions by splitting features.

```
         Does alcohol > 12.5?
              /        \
            YES         NO
             /           \
     Flavanoids>2.2?   Phenols>1.8?
       /        \        /        \
     Class1   Class2   Class3    Class1
```

#### What is Random Forest?
An **ensemble** of many decision trees voting together.

#### Key Characteristics:
1. **Multiple Trees**: 100 trees in our model
2. **Random Sampling**: Each tree uses random subset of data
3. **Random Features**: Each split uses random features
4. **Voting**: Final prediction = majority vote

#### Why Random Forest Works:
- **Reduces Overfitting**: Individual trees overfit, ensemble averages them
- **Handles Nonlinearity**: Can model complex patterns
- **Feature Importance**: Tells which features matter most
- **Multiclass Ready**: Naturally handles 3+ classes

#### In Our Project:
```python
from sklearn.ensemble import RandomForestClassifier

rf_model = RandomForestClassifier(
    n_estimators=100,        # 100 trees
    max_depth=10,            # Max tree depth (prevent overfitting)
    min_samples_split=5,     # Min samples to split node
    min_samples_leaf=2,      # Min samples in leaf node
    random_state=42,         # Reproducibility
    n_jobs=-1                # Use all CPU cores
)

rf_model.fit(X_train, y_train)
```

#### Feature Importance (from our trained model):
Which features matter most for prediction?

```
Feature              Importance
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
color_intensity      0.3245  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
flavanoids          0.2158  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
hue                 0.1876  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
proline             0.1654  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
total_phenols       0.0789  â–ˆâ–ˆâ–ˆâ–ˆ
alcohol             0.0278  â–ˆ
```

**Interpretation**: Color intensity is 3x more important than alcohol for predicting cultivar.

---

### 5. MODEL EVALUATION METRICS

#### Why Multiple Metrics?
Accuracy alone can be misleading. Example:

```
True Labels:    [1, 1, 1, 2, 2, 3]
Predictions:    [1, 1, 1, 2, 2, 2]  â† Wrong on last sample

Accuracy = 5/6 = 83%  â† Looks good!
But we missed Class 3 completely!
```

#### The Confusion Matrix
```
               Predicted Class
             Class1  Class2  Class3
Actual  Class1  [11]    [0]    [0]
Class   Class2  [0]    [12]    [1]
        Class3  [0]    [0]     [12]
```

**Reading**: Diagonal = correct predictions, off-diagonal = errors

#### Key Metrics Explained

**1. Accuracy** = (All Correct) / (Total)
```
Formula: (TP + TN) / (TP + TN + FP + FN)
Meaning: Overall correctness
Range: 0 to 1 (0% to 100%)
Example: 97% correct predictions
When to Use: When classes are balanced
```

**2. Precision** = (Correct Positive) / (All Predicted Positive)
```
Formula: TP / (TP + FP)
Meaning: Of predictions I made, how many were right?
Range: 0 to 1
Example: 98% of "Class 1" predictions were actually Class 1
When to Use: Cost of false positives is high
```

**3. Recall** = (Correct Positive) / (All Actual Positive)
```
Formula: TP / (TP + FN)
Meaning: Of actual positives, how many did I find?
Range: 0 to 1
Example: Found 96% of all Class 1 samples
When to Use: Cost of missing positives is high
```

**4. F1-Score** = Harmonic Mean of Precision & Recall
```
Formula: 2 * (Precision * Recall) / (Precision + Recall)
Meaning: Balance between precision and recall
Range: 0 to 1
Example: 97% balanced score
When to Use: Want to balance false positives and false negatives
```

#### Our Model's Metrics

```
                 Precision  Recall  F1-Score  Support
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Cultivar 1         0.97     0.97     0.97      11
Cultivar 2         0.97     0.97     0.97      12
Cultivar 3         0.97     0.97     0.97      13
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Weighted Avg       0.97     0.97     0.97      36
```

**Interpretation**: Model performs excellently across all cultivars with no bias.

---

### 6. MODEL PERSISTENCE (Saving/Loading)

#### Why Save Models?
- Don't retrain every time you use the model
- Saves time (training takes minutes, loading takes seconds)
- Ensures consistent predictions
- Can deploy to production

#### Joblib vs Pickle

| Feature | Joblib | Pickle |
|---------|--------|--------|
| Efficiency | Excellent for NumPy | Good but slower |
| File Size | Smaller | Larger |
| Speed | Faster | Slower |
| Common Use | ML models | General Python objects |

#### In Our Project:
```python
import joblib

# SAVING
joblib.dump(rf_model, 'wine_cultivar_model.pkl')
joblib.dump(scaler, 'scaler.pkl')
joblib.dump(selected_features, 'features.pkl')

# LOADING
model = joblib.load('wine_cultivar_model.pkl')
scaler = joblib.load('scaler.pkl')
features = joblib.load('features.pkl')
```

#### Why Save 3 Files?
```
1. Model (.pkl) - The trained Random Forest
2. Scaler (.pkl) - The fitted StandardScaler
3. Features (.pkl) - List of feature names

Why?
- Must apply SAME scaler as training
- Must use SAME features in SAME order
- Different scaler = different predictions
```

---

### 7. WEB DEPLOYMENT WITH STREAMLIT

#### What is Streamlit?
Framework to build web apps without HTML/CSS/JavaScript.

#### Key Streamlit Components:

**1. Input Components**
```python
# Number input
alcohol = st.number_input("Alcohol (%)", value=12.5)

# Text input
name = st.text_input("Wine Name")

# Dropdown
cultivar = st.selectbox("Expected Cultivar", [1, 2, 3])
```

**2. Display Components**
```python
st.write("Hello World")          # Plain text
st.title("Big Title")            # Heading
st.markdown("**Bold** text")      # Markdown
st.error("Error message")        # Error box
st.success("Success!")           # Success box
st.info("Information")           # Info box
```

**3. Data Components**
```python
st.table(dataframe)              # Display table
st.bar_chart(dataframe)          # Bar chart
st.line_chart(dataframe)         # Line chart
st.write(confusion_matrix)       # Write any object
```

**4. Layout Components**
```python
col1, col2 = st.columns(2)       # Two columns
with col1:
    st.write("Left column")
with col2:
    st.write("Right column")

tab1, tab2 = st.tabs(["Tab 1", "Tab 2"])
with tab1:
    st.write("Tab 1 content")
```

#### Our Application Structure:
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Wine Cultivar Prediction      â”‚  â† Title
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ [ğŸ”® Prediction] [ğŸ“Š About] [â„¹ï¸ Info] â”‚  â† Tabs
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Input Fields:                  â”‚
â”‚  â”œâ”€ Alcohol                     â”‚
â”‚  â”œâ”€ Total Phenols              â”‚
â”‚  â”œâ”€ Flavanoids                 â”‚
â”‚  â”œâ”€ Color Intensity            â”‚
â”‚  â”œâ”€ Hue                         â”‚
â”‚  â””â”€ Proline                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ [ğŸ” Predict Cultivar]           â”‚  â† Button
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Result:                         â”‚
â”‚ âœ“ Predicted: Cultivar 1         â”‚
â”‚ âœ“ Confidence: 95%              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ’¡ EXAM QUESTIONS & ANSWERS

### Question 1: Why use stratified train-test split?
**Answer**: To ensure test set has same class distribution as original data. Prevents biased evaluation when classes are imbalanced.

### Question 2: What does StandardScaler do?
**Answer**: Transforms features to mean=0, std=1. Important because features have different ranges (alcohol 10-15 vs proline 300-1700).

### Question 3: Why Random Forest for this problem?
**Answer**: 
- Handles multiclass naturally
- Reduces overfitting through ensemble
- Gives feature importance
- Works well with mixed feature types
- Non-linear relationships

### Question 4: Difference between Precision and Recall?
**Answer**: 
- **Precision**: Of predictions made, how many correct? (False positive focus)
- **Recall**: Of actual samples, how many found? (False negative focus)

### Question 5: Why save scaler separately?
**Answer**: Must apply SAME transformation to new data as training data. Different scaler = garbage predictions.

### Question 6: What's advantage of Joblib over Pickle?
**Answer**: More efficient with NumPy objects, smaller file sizes, faster loading for ML models.

### Question 7: How does Random Forest prevent overfitting?
**Answer**: 
- Multiple trees average out
- Random sampling reduces variance
- Max depth limits tree complexity
- Not dependent on single feature

### Question 8: What does Feature Importance tell us?
**Answer**: Which features most impact predictions. Color intensity (32%) matters more than alcohol (2.7%).

### Question 9: Why test set matters?
**Answer**: Training metrics can be misleading. Test set evaluates on unseen data to catch overfitting.

### Question 10: How to interpret confusion matrix diagonal?
**Answer**: Diagonal = correct predictions. Off-diagonal = errors. Higher diagonal = better model.

---

## ğŸ”§ CODE SNIPPETS FOR REFERENCE

### Full Pipeline in 20 Lines:
```python
from sklearn.datasets import load_wine
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
import joblib

# 1. Load and prepare
wine = load_wine()
X, y = wine.data[:, [0,6,7,8,9,12]], wine.target

# 2. Scale
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# 3. Split
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.2, stratify=y, random_state=42)

# 4. Train
model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# 5. Evaluate
accuracy = accuracy_score(y_test, model.predict(X_test))
print(f"Accuracy: {accuracy:.4f}")

# 6. Save
joblib.dump(model, 'model.pkl')
joblib.dump(scaler, 'scaler.pkl')
```

### Making Predictions:
```python
import joblib
import pandas as pd

# Load
model = joblib.load('model.pkl')
scaler = joblib.load('scaler.pkl')

# New sample
new_data = [[12.5, 2.3, 2.2, 4.5, 1.0, 600]]
df_new = pd.DataFrame(new_data, columns=['alcohol', 'phenols', ...])

# Scale & Predict
X_scaled = scaler.transform(df_new)
prediction = model.predict(X_scaled)  # [0] = Class 0
probability = model.predict_proba(X_scaled)
# [[0.02, 0.95, 0.03]] = 95% Class 1

print(f"Cultivar: {prediction[0]}")
print(f"Confidence: {probability[0].max():.2%}")
```

---

## ğŸ“Š KEY FORMULAS

### Accuracy
$$\text{Accuracy} = \frac{TP + TN}{TP + TN + FP + FN}$$

### Precision
$$\text{Precision} = \frac{TP}{TP + FP}$$

### Recall
$$\text{Recall} = \frac{TP}{TP + FN}$$

### F1-Score
$$\text{F1} = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}$$

### StandardScaler
$$X_{scaled} = \frac{X - \mu}{\sigma}$$

### Weighted Average
$$\text{Weighted} = \frac{\sum(\text{Metric}_i \times \text{Support}_i)}{\sum \text{Support}_i}$$

---

## ğŸ“– STUDY TIPS FOR EXAM

1. **Understand the Pipeline**: Data â†’ Preprocessing â†’ Train â†’ Evaluate â†’ Deploy
2. **Know the Formulas**: Memorize accuracy, precision, recall, F1
3. **Understand the Concept**: Don't just memorize, understand WHY
4. **Practice Calculations**: Calculate metrics manually on paper
5. **Code Review**: Go through code line-by-line, understand what each does
6. **Teach Someone**: Explain the project to a friend
7. **Review Notebooks**: Run cells one-by-one and observe outputs
8. **Make Predictions**: Try different inputs and interpret results

---

## ğŸ¯ FINAL CHECKLIST

Before exam, ensure you know:

- [ ] What is data preprocessing and why it's important
- [ ] How StandardScaler works and why it's mandatory
- [ ] Train-test split concept and stratification
- [ ] What Random Forest is and how it works
- [ ] All 4 metrics: accuracy, precision, recall, F1
- [ ] How to read and interpret confusion matrix
- [ ] Why save model as separate file
- [ ] Joblib vs Pickle differences
- [ ] Basic Streamlit components and syntax
- [ ] How to load model and make predictions
- [ ] What each line of code does in the project

---

**Good luck with your exam! ğŸ€**
*This project covers most key ML concepts taught in CSC 415.*
